import SwiftUI

struct ContentView: View {
    @StateObject private var whisperState = WhisperState()
    @StateObject private var openAIManager = OpenAIManager()
    @StateObject private var elevenLabsManager = ElevenLabsManager()

    var body: some View {
        VStack {
            ScrollView {
                Text(whisperState.messageLog)
                    .padding()
            }
            .frame(maxWidth: .infinity, maxHeight: .infinity)
        }
        .onAppear {
            startSession()
        }
    }
    
    private func startSession() {
        whisperState.messageLog = "Welcome to the Tutor App, I will teach you how to count by tens. Can you count 1 2 3?"
        Task {
            await elevenLabsManager.speakText("Welcome to the Tutor App, I will teach you how to count by tens. Can you count 1 2 3?")
            await listenAndRespond()
        }
    }

    private func listenAndRespond() async {
        await whisperState.startRecording()
        try? await Task.sleep(nanoseconds: 10_000_000_000) // Simulate 10 seconds of recording
        await whisperState.stopRecording()
        
        guard let transcribedText = whisperState.transcribedText else {
            whisperState.messageLog += "\nNo transcription available."
            await listenAndRespond()
            return
        }
        
        let response = await openAIManager.sendTextToOpenAI(transcribedText)
        whisperState.messageLog += "\nAssistant: \(response)"
        
        await elevenLabsManager.speakText(response)
        await listenAndRespond() // Recursively call to continue the process
    }
}

@main
struct WhisperApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
    }
}

import SwiftUI

//@main
struct WhisperCppDemoApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
    }
}

import Foundation

func decodeWaveFile(_ url: URL) throws -> [Float] {
    let data = try Data(contentsOf: url)
    let floats = stride(from: 44, to: data.count, by: 2).map {
        return data[$0..<$0 + 2].withUnsafeBytes {
            let short = Int16(littleEndian: $0.load(as: Int16.self))
            return max(-1.0, min(Float(short) / 32767.0, 1.0))
        }
    }
    return floats
}

import Foundation

class OpenAIManager: ObservableObject {
    private let apiKey: String

    init() {
        guard let apiKey = Bundle.main.object(forInfoDictionaryKey: "OpenAIAPIKey") as? String else {
            fatalError("API key not found in Info.plist")
        }
        self.apiKey = apiKey
    }

    func sendTextToOpenAI(_ text: String) async -> String {
        let prompt = """
        You are Socrates, an AI TUTOR trying to teach a 2-7 year old how to count in tens such as 10, 20, 30, 40, 50 etc. Given the previous context of the conversation the child has said "\(text)". Give a prompt to the child back as to how to learn to count back in tens. Take many cycles to do the counting examples, before moving to more elaborate examples. In each response only do one activity following up on the previous activity. ONLY EVER GIVE ONE SINGLE RESPONSE, as you RESPONSE is directly being read out loud ot a kid.
        """
        
        let requestData: [String: Any] = [
            "model": "gpt-4",
            "messages": [["role": "user", "content": prompt]],
            "max_tokens": 150
        ]

        guard let url = URL(string: "https://api.openai.com/v1/chat/completions") else {
            return "Invalid URL"
        }

        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")
        request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")

        do {
            request.httpBody = try JSONSerialization.data(withJSONObject: requestData, options: [])
        } catch {
            return "Failed to encode request data: \(error.localizedDescription)"
        }

        do {
            let (data, response) = try await URLSession.shared.data(for: request)
            
            guard let httpResponse = response as? HTTPURLResponse, httpResponse.statusCode == 200 else {
                return "Failed request with response: \(response)"
            }
            
            guard let responseData = try? JSONSerialization.jsonObject(with: data, options: []) as? [String: Any],
                  let choices = responseData["choices"] as? [[String: Any]],
                  let message = choices.first?["message"] as? [String: Any],
                  let content = message["content"] as? String else {
                return "Failed to decode response data"
            }
            
            return content
        } catch {
            return "Failed to make OpenAI request: \(error.localizedDescription)"
        }
    }
}

import Foundation
import AVFoundation

class ElevenLabsManager: NSObject, ObservableObject, AVAudioPlayerDelegate {
    @Published var isSpeaking = false
    
    private var audioPlayer: AVAudioPlayer?
    private let xiApiKey: String
    private let voiceId: String
    
    override init() {
        guard let apiKey = Bundle.main.object(forInfoDictionaryKey: "ElevenLabsAPIKey") as? String,
              let voiceId = Bundle.main.object(forInfoDictionaryKey: "ElevenLabsVoiceID") as? String else {
            fatalError("API Key or Voice ID not found in Info.plist")
        }
        self.xiApiKey = apiKey
        self.voiceId = voiceId
    }
    
    func speakText(_ text: String) async {
        let url = URL(string: "https://api.elevenlabs.io/v1/text-to-speech/\(voiceId)")!
        
        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        request.addValue("application/json", forHTTPHeaderField: "Content-Type")
        request.addValue(xiApiKey, forHTTPHeaderField: "xi-api-key")
        
        let body: [String: Any] = [
            "text": text,
            "model_id": "eleven_monolingual_v1",
            "voice_settings": [
                "stability": 0.5,
                "similarity_boost": 0.5
            ]
        ]
        
        request.httpBody = try? JSONSerialization.data(withJSONObject: body, options: [])
        
        let (data, response) = try! await URLSession.shared.data(for: request)
        
        guard let httpResponse = response as? HTTPURLResponse, httpResponse.statusCode == 200 else {
            print("Failed request with response: \(response)")
            return
        }
        
        DispatchQueue.main.async {
            self.isSpeaking = true
        }
        
        audioPlayer = try? AVAudioPlayer(data: data)
        audioPlayer?.delegate = self
        audioPlayer?.play()
    }
    
    func audioPlayerDidFinishPlaying(_ player: AVAudioPlayer, successfully flag: Bool) {
        DispatchQueue.main.async {
            self.isSpeaking = false
        }
    }
}

import Foundation
import SwiftUI
import AVFoundation
import Combine

@MainActor
class WhisperState: NSObject, ObservableObject {
    @Published var isModelLoaded = false
    @Published var messageLog = ""
    @Published var canTranscribe = false
    @Published var isRecording = false
    @Published var transcribedText: String?

    private var whisperContext: WhisperContext?
    private var audioEngine: AVAudioEngine?
    private var audioInputNode: AVAudioInputNode?
    private var audioConverter: AVAudioConverter?
    private var accumulatedBuffer: AVAudioPCMBuffer?
    private var conversationHistory: [String] = []
    var openAIManager: OpenAIManager?
    var elevenLabsManager: ElevenLabsManager?

    override init() {
        super.init()
        do {
            try loadModel()
            canTranscribe = true
        } catch {
            print(error.localizedDescription)
            DispatchQueue.main.async {
                self.messageLog += "\(error.localizedDescription)\n"
            }
        }
    }

    private func loadModel() throws {
        DispatchQueue.main.async {
            self.messageLog += "Loading model...\n"
        }
        if let modelUrl = Bundle.main.url(forResource: "ggml-tiny.en", withExtension: "bin", subdirectory: "models") {
            whisperContext = try WhisperContext.createContext(path: modelUrl.path())
            DispatchQueue.main.async {
                self.messageLog += "Loaded model \(modelUrl.lastPathComponent)\n"
            }
        } else {
            DispatchQueue.main.async {
                self.messageLog += "Could not locate model\n"
            }
        }
    }

    func startRecording() async {
        requestRecordPermission { granted in
            if granted {
                Task {
                    do {
                        try self.beginRecording()
                        try await Task.sleep(nanoseconds: 10_000_000_000) // Record for 10 seconds
                        await self.stopRecording()
                    } catch {
                        print(error.localizedDescription)
                        DispatchQueue.main.async {
                            self.messageLog += "\(error.localizedDescription)\n"
                            self.isRecording = false
                        }
                    }
                }
            } else {
                DispatchQueue.main.async {
                    self.messageLog += "Record permission not granted.\n"
                }
            }
        }
    }

    private func beginRecording() throws {
        let audioSession = AVAudioSession.sharedInstance()
        try audioSession.setCategory(.playAndRecord, mode: .default, options: [.defaultToSpeaker, .allowBluetooth])
        try audioSession.setActive(true, options: .notifyOthersOnDeactivation)

        audioEngine = AVAudioEngine()
        audioInputNode = audioEngine?.inputNode

        guard let inputFormat = audioInputNode?.inputFormat(forBus: 0) else {
            throw NSError(domain: "WhisperState", code: -1, userInfo: [NSLocalizedDescriptionKey: "Failed to get input format"])
        }

        let outputFormat = AVAudioFormat(commonFormat: .pcmFormatFloat32, sampleRate: 16000, channels: 1, interleaved: false)
        audioConverter = AVAudioConverter(from: inputFormat, to: outputFormat!)

        let bufferSize: AVAudioFrameCount = 1024
        accumulatedBuffer = AVAudioPCMBuffer(pcmFormat: outputFormat!, frameCapacity: 16000 * 10)! // Buffer to accumulate 10 seconds of audio

        guard accumulatedBuffer != nil else {
            throw NSError(domain: "WhisperState", code: -1, userInfo: [NSLocalizedDescriptionKey: "Failed to create PCM buffer"])
        }

        audioInputNode?.installTap(onBus: 0, bufferSize: bufferSize, format: inputFormat) { [weak self] (buffer, time) in
            guard let self = self else { return }
            let inputBlock: AVAudioConverterInputBlock = { _, status in
                status.pointee = .haveData
                return buffer
            }
            var error: NSError?
            let convertedBuffer = AVAudioPCMBuffer(pcmFormat: outputFormat!, frameCapacity: bufferSize)
            self.audioConverter?.convert(to: convertedBuffer!, error: &error, withInputFrom: inputBlock)
            if let convertedBuffer = convertedBuffer, error == nil {
                self.appendBuffer(convertedBuffer)
            }
        }

        try audioEngine?.start()
        isRecording = true
        DispatchQueue.main.async {
            self.messageLog += "Recording started...\n"
        }
    }

    private func appendBuffer(_ buffer: AVAudioPCMBuffer) {
        guard let accumulatedBuffer else { return }

        let availableFrames = accumulatedBuffer.frameCapacity - accumulatedBuffer.frameLength
        let framesToCopy = min(buffer.frameLength, availableFrames)
        guard framesToCopy > 0 else { return }

        let sourcePointer = buffer.floatChannelData![0]
        let destinationPointer = accumulatedBuffer.floatChannelData![0].advanced(by: Int(accumulatedBuffer.frameLength))

        memcpy(destinationPointer, sourcePointer, Int(framesToCopy) * MemoryLayout<Float>.size)
        accumulatedBuffer.frameLength += framesToCopy
    }

    func stopRecording() async {
        audioEngine?.stop()
        audioInputNode?.removeTap(onBus: 0)
        isRecording = false
        if let buffer = accumulatedBuffer {
            await transcribeAudio(buffer)
        }
    }

    private func requestRecordPermission(response: @escaping (Bool) -> Void) {
        AVAudioSession.sharedInstance().requestRecordPermission { granted in
            response(granted)
        }
    }

    private func transcribeAudio(_ buffer: AVAudioPCMBuffer) async {
        if (!canTranscribe) {
            return
        }
        guard let whisperContext else {
            return
        }

        canTranscribe = false
        DispatchQueue.main.async {
            self.messageLog += "Transcribing data...\n"
        }
        let data = Array(UnsafeBufferPointer(start: buffer.floatChannelData![0], count: Int(buffer.frameLength)))
        print("Transcribing data with length: \(data.count)")
        await whisperContext.fullTranscribe(samples: data)
        let text = await whisperContext.getTranscription()
        DispatchQueue.main.async {
            self.messageLog += "Done: \(text)\n"
            self.transcribedText = text
            self.conversationHistory.append("User: \(text)")
        }

        canTranscribe = true
    }
}

