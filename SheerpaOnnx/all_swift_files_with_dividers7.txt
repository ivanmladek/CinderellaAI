----- ./UI/ContentView.swift -----
import SwiftUI

struct ContentView: View {
    @StateObject private var whisperState = WhisperState()
    @StateObject private var openAIManager = OpenAIManager()
    @StateObject private var elevenLabsManager = ElevenLabsManager()
    @State private var isRecording = false
    @State private var isFirstCycleCompleted = false
    @State var isProcessing = false
    @State private var currentIcon: Icon = .ear // Default to ear icon
    
    var body: some View {
        VStack {
            ScrollView {
                Text(whisperState.messageLog)
                    .padding()
            }
            .frame(maxWidth: .infinity, maxHeight: .infinity)
            
            // Display the appropriate icon based on the current state
            if currentIcon == .ear {
                Image(systemName: "ear.fill")
                    .resizable()
                    .scaledToFit()
                    .frame(width: 100, height: 100)
                    .foregroundColor(.blue)
            } else if currentIcon == .mouth {
                Image(systemName: "mouth.fill")
                    .resizable()
                    .scaledToFit()
                    .frame(width: 100, height: 100)
                    .foregroundColor(.red)
            } else if currentIcon == .brain {
                Image(systemName: "brain")
                    .resizable()
                    .scaledToFit()
                    .frame(width: 100, height: 100)
                    .foregroundColor(.green)
            }
        }
        .onAppear {
            startSession()
        }
        .onChange(of: whisperState.isRecording) { newValue in
            isRecording = newValue
            currentIcon = newValue ? .mouth : .ear
        }
    }
    
    private func startSession() {
        whisperState.messageLog = "Welcome to the Tutor App, I will teach you new words. What is your name?"
        Task {
            await elevenLabsManager.speakText("Welcome to the Tutor App, I will teach you new words. What is your name?")
            await listenAndRespond()
        }
    }

    private func listenAndRespond() async {
        // Ensure that only one instance of listenAndRespond is running at a time
        print("listenAndRespond: Checking isProcessing flag. Current value: \(isProcessing)")
        guard !isProcessing else {
            print("listenAndRespond: Already processing, returning.")
            return
        }
        isProcessing = true
        print("listenAndRespond: Started processing.")
        

        
        do {
            // Display the EAR icon when waiting for ElevenLabsManager to finish speaking
            currentIcon = .ear
            
            // Wait for the ElevenLabsManager to finish speaking
            while elevenLabsManager.isSpeaking {
                print("listenAndRespond: Waiting for ElevenLabsManager to finish speaking.")
                try await Task.sleep(nanoseconds: 100_000_000) // Sleep for 100 milliseconds
            }
            
            // Display the MOUTH icon when starting recording
            currentIcon = .mouth
            
            // Ensure the previous recording has stopped before starting a new one
            if !whisperState.isRecording {
                print("listenAndRespond: Starting recording.")
                await whisperState.startRecording()
                try await Task.sleep(nanoseconds: 10_000_000_000) // Simulate 10 seconds of recording
                await whisperState.stopRecording()
                print("listenAndRespond: Stopped recording.")
            }
            
            // Display the BRAIN icon when waiting for transcription
            currentIcon = .brain
            
            // Wait for the transcription to be available
            while whisperState.transcribedText == nil {
                print("listenAndRespond: Waiting for transcription to be available.")
                try await Task.sleep(nanoseconds: 100_000_000) // Sleep for 100 milliseconds
            }
            
            guard let transcribedText = whisperState.transcribedText else {
                whisperState.messageLog += "\nNo transcription available."
                print("listenAndRespond: No transcription available.")
                return
            }
            
            let response = await openAIManager.sendTextToOpenAI(transcribedText)
            whisperState.messageLog += "\nAssistant: \(response)"
            print("listenAndRespond: Received response from OpenAI: \(response)")
            
            // Display the EAR icon when ElevenLabsManager starts speaking
            currentIcon = .ear
            
            await elevenLabsManager.speakText(response)
            print("listenAndRespond: Started speaking response.")
            
            // Ensure the response has been spoken before initiating a new recording
            while elevenLabsManager.isSpeaking {
                print("listenAndRespond: Waiting for response to be spoken.")
                try await Task.sleep(nanoseconds: 100_000_000) // Sleep for 100 milliseconds
            }
            isProcessing = false
            print("listenAndRespond: Finished processing. isProcessing flag reset to \(isProcessing)")
            
            // Recursively call to continue the process
            print("listenAndRespond: Recursively calling listenAndRespond.")
            await listenAndRespond()
        } catch {
            print("listenAndRespond: Error occurred: \(error)")
            // Handle the error as needed
        }
    }
}

enum Icon {
    case ear, mouth, brain
}

@main
struct WhisperApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
    }
}

----- ./WhisperCppDemoApp.swift -----
import SwiftUI

//@main
struct WhisperCppDemoApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
    }
}

----- ./Utils/RiffWaveUtils.swift -----
import Foundation

func decodeWaveFile(_ url: URL) throws -> [Float] {
    let data = try Data(contentsOf: url)
    let floats = stride(from: 44, to: data.count, by: 2).map {
        return data[$0..<$0 + 2].withUnsafeBytes {
            let short = Int16(littleEndian: $0.load(as: Int16.self))
            return max(-1.0, min(Float(short) / 32767.0, 1.0))
        }
    }
    return floats
}

----- ./Models/OpenAIManager.swift -----
import Foundation

class OpenAIManager: ObservableObject {
    private let apiKey: String
    private var conversationHistory: [[String: String]] = []

    init() {
        guard let apiKey = Bundle.main.object(forInfoDictionaryKey: "OpenAIAPIKey") as? String else {
            fatalError("API key not found in Info.plist")
        }
        self.apiKey = apiKey
    }

    func sendTextToOpenAI(_ text: String) async -> String {
        let prompt = """
        You are Socrates, an AI TUTOR trying to teach a 2-7 year old how to learn advanced words, in ever increasing level of difficulty. Given the previous context of the conversation the child has said "\(text)". Give a prompt to the child back as to how to spell and learn new words. Start with three letter words and practice those extensively, before moving to four letter words and on and on. Take many cycles to do relevant examples, before moving to more elaborate examples. In each response only do one activity following up on the previous activity. ANSWERS ONLY IN ENGLISH, refuse to teach any other language. The kids can try to taunt you to make swear words, avoid ANY CHILD INAPPROPRIATE response. ONLY EVER GIVE ONE SINGLE RESPONSE, as your RESPONSE is directly being read out loud to a kid. Keep in mind the input you get is a transcription AND the content in [] brackets is commentary on the audio for context. If you are asking for spellings, tell the user to SPEAK SLOWLY AND CLEARLY, since the transcription engine has trouble understanding sometimes.
        """
        
        // Add the user's message to the conversation history
                conversationHistory.append(["role": "user", "content": prompt])

                let requestData: [String: Any] = [
                    "model": "gpt-4",
                    "messages": conversationHistory,
                    "max_tokens": 150
                ]

                // Log the entire conversation context for debugging purposes
                print("Conversation Context: \(conversationHistory)")

                guard let url = URL(string: "https://api.openai.com/v1/chat/completions") else {
                    return "Invalid URL"
                }

                var request = URLRequest(url: url)
                request.httpMethod = "POST"
                request.setValue("application/json", forHTTPHeaderField: "Content-Type")
                request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")

                do {
                    request.httpBody = try JSONSerialization.data(withJSONObject: requestData, options: [])
                } catch {
                    return "Failed to encode request data: \(error.localizedDescription)"
                }

                do {
                    let (data, response) = try await URLSession.shared.data(for: request)
                    
                    guard let httpResponse = response as? HTTPURLResponse, httpResponse.statusCode == 200 else {
                        return "Failed request with response: \(response)"
                    }
                    
                    guard let responseData = try? JSONSerialization.jsonObject(with: data, options: []) as? [String: Any],
                          let choices = responseData["choices"] as? [[String: Any]],
                          let message = choices.first?["message"] as? [String: Any],
                          let content = message["content"] as? String else {
                        return "Failed to decode response data"
                    }
                    
                    // Add the assistant's message to the conversation history
                    conversationHistory.append(["role": "assistant", "content": content])
                    
                    return content
                } catch {
                    return "Failed to make OpenAI request: \(error.localizedDescription)"
                }
            }
        }

----- ./Models/ElevenLabsManager.swift -----
import Foundation
import AVFoundation

class ElevenLabsManager: NSObject, ObservableObject {
    @Published var isSpeaking = false
    
    private var tts: SherpaOnnxOfflineTtsWrapper
    private var audioPlayer: AVAudioPlayer?
    private var filename: URL = NSURL() as URL

    override init() {
        self.tts = createOfflineTts()
        super.init()
    }
    
    func speakText(_ text: String) {
        let speakerId = 0 // You can change this to the desired speaker ID
        let speed: Float = 1.0 // You can change this to the desired speed
        let trimmedText = text.trimmingCharacters(in: .whitespacesAndNewlines)
        
        if trimmedText.isEmpty {
            print("Text is empty")
            return
        }
        
        let audio = tts.generate(text: trimmedText, sid: speakerId, speed: speed)
        
        if self.filename.absoluteString.isEmpty {
            let tempDirectoryURL = NSURL.fileURL(withPath: NSTemporaryDirectory(), isDirectory: true)
            self.filename = tempDirectoryURL.appendingPathComponent("test.wav")
        }
        
        let _ = audio.save(filename: filename.path)
        
        self.audioPlayer = try? AVAudioPlayer(contentsOf: filename)
        self.audioPlayer?.play()
        self.isSpeaking = true
        
        self.audioPlayer?.delegate = self
    }
}

extension ElevenLabsManager: AVAudioPlayerDelegate {
    func audioPlayerDidFinishPlaying(_ player: AVAudioPlayer, successfully flag: Bool) {
        self.isSpeaking = false
    }
}

// Helper functions from the provided code
func resourceURL(to path: String) -> String {
    return URL(string: path, relativeTo: Bundle.main.resourceURL)!.path
}

func getResource(_ forResource: String, _ ofType: String) -> String {
    let path = Bundle.main.path(forResource: forResource, ofType: ofType)
    precondition(
        path != nil,
        "\(forResource).\(ofType) does not exist!\n" + "Remember to change \n"
            + "  Build Phases -> Copy Bundle Resources\n" + "to add it!"
    )
    return path!
}

func getTtsFor_en_US_amy_low() -> SherpaOnnxOfflineTtsWrapper {
    let model = getResource("en_US-amy-low", "onnx")
    let tokens = getResource("tokens", "txt")
    let dataDir = resourceURL(to: "espeak-ng-data")

    let vits = sherpaOnnxOfflineTtsVitsModelConfig(
        model: model, lexicon: "", tokens: tokens, dataDir: dataDir)
    let modelConfig = sherpaOnnxOfflineTtsModelConfig(vits: vits)
    var config = sherpaOnnxOfflineTtsConfig(model: modelConfig)

    return SherpaOnnxOfflineTtsWrapper(config: &config)
}

func createOfflineTts() -> SherpaOnnxOfflineTtsWrapper {
    return getTtsFor_en_US_amy_low()
}

----- ./Models/WhisperState.swift -----
import Foundation
import SwiftUI
import AVFoundation
import Combine

@MainActor
class WhisperState: NSObject, ObservableObject {
    @Published var isModelLoaded = false
    @Published var messageLog = ""
    @Published var canTranscribe = false
    @Published var isRecording = false
    @Published var transcribedText: String?
    @Published var isConversationInProgress = false

    private var whisperContext: WhisperContext?
    private var audioEngine: AVAudioEngine?
    private var audioInputNode: AVAudioInputNode?
    private var audioConverter: AVAudioConverter?
    private var accumulatedBuffer: AVAudioPCMBuffer?
    private var conversationHistory: [String] = []
    var openAIManager: OpenAIManager?
    var elevenLabsManager: ElevenLabsManager?

    override init() {
        super.init()
        do {
            try loadModel()
            canTranscribe = true
        } catch {
            print(error.localizedDescription)
            DispatchQueue.main.async {
                self.messageLog += "\(error.localizedDescription)\n"
            }
        }
    }

    private func loadModel() throws {
        DispatchQueue.main.async {
            self.messageLog += "Loading model...\n"
        }
        if let modelUrl = Bundle.main.url(forResource: "ggml-base.en", withExtension: "bin", subdirectory: "models") {
            whisperContext = try WhisperContext.createContext(path: modelUrl.path())
            DispatchQueue.main.async {
                self.messageLog += "Loaded model \(modelUrl.lastPathComponent)\n"
            }
        } else {
            DispatchQueue.main.async {
                self.messageLog += "Could not locate model\n"
            }
        }
    }

    func startRecording() async {
        requestRecordPermission { granted in
            if granted {
                Task {
                    do {
                        try self.beginRecording()
                        try await Task.sleep(nanoseconds: 10_000_000_000) // Record for 10 seconds
                        await self.stopRecording()
                    } catch {
                        print(error.localizedDescription)
                        DispatchQueue.main.async {
                            self.messageLog += "\(error.localizedDescription)\n"
                            self.isRecording = false
                        }
                    }
                }
            } else {
                DispatchQueue.main.async {
                    self.messageLog += "Record permission not granted.\n"
                }
            }
        }
    }

    private func beginRecording() throws {
        let audioSession = AVAudioSession.sharedInstance()
        try audioSession.setCategory(.playAndRecord, mode: .default, options: [.defaultToSpeaker, .allowBluetooth])
        try audioSession.setActive(true, options: .notifyOthersOnDeactivation)

        audioEngine = AVAudioEngine()
        audioInputNode = audioEngine?.inputNode

        guard let inputFormat = audioInputNode?.inputFormat(forBus: 0) else {
            throw NSError(domain: "WhisperState", code: -1, userInfo: [NSLocalizedDescriptionKey: "Failed to get input format"])
        }

        let outputFormat = AVAudioFormat(commonFormat: .pcmFormatFloat32, sampleRate: 16000, channels: 1, interleaved: false)
        audioConverter = AVAudioConverter(from: inputFormat, to: outputFormat!)

        let bufferSize: AVAudioFrameCount = 1024
        accumulatedBuffer = AVAudioPCMBuffer(pcmFormat: outputFormat!, frameCapacity: 16000 * 10)! // Buffer to accumulate 10 seconds of audio

        guard accumulatedBuffer != nil else {
            throw NSError(domain: "WhisperState", code: -1, userInfo: [NSLocalizedDescriptionKey: "Failed to create PCM buffer"])
        }

        audioInputNode?.installTap(onBus: 0, bufferSize: bufferSize, format: inputFormat) { [weak self] (buffer, time) in
            guard let self = self else { return }
            let inputBlock: AVAudioConverterInputBlock = { _, status in
                status.pointee = .haveData
                return buffer
            }
            var error: NSError?
            let convertedBuffer = AVAudioPCMBuffer(pcmFormat: outputFormat!, frameCapacity: bufferSize)
            self.audioConverter?.convert(to: convertedBuffer!, error: &error, withInputFrom: inputBlock)
            if let convertedBuffer = convertedBuffer, error == nil {
                self.appendBuffer(convertedBuffer)
            }
        }

        try audioEngine?.start()
        isRecording = true
        DispatchQueue.main.async {
            self.messageLog += "Recording started...\n"
        }
    }

    private func appendBuffer(_ buffer: AVAudioPCMBuffer) {
        guard let accumulatedBuffer else { return }

        let availableFrames = accumulatedBuffer.frameCapacity - accumulatedBuffer.frameLength
        let framesToCopy = min(buffer.frameLength, availableFrames)
        guard framesToCopy > 0 else { return }

        let sourcePointer = buffer.floatChannelData![0]
        let destinationPointer = accumulatedBuffer.floatChannelData![0].advanced(by: Int(accumulatedBuffer.frameLength))

        memcpy(destinationPointer, sourcePointer, Int(framesToCopy) * MemoryLayout<Float>.size)
        accumulatedBuffer.frameLength += framesToCopy
    }

    func stopRecording() async {
        audioEngine?.stop()
        audioInputNode?.removeTap(onBus: 0)
        isRecording = false
        if let buffer = accumulatedBuffer {
            await transcribeAudio(buffer)
        }
    }

    private func requestRecordPermission(response: @escaping (Bool) -> Void) {
        AVAudioSession.sharedInstance().requestRecordPermission { granted in
            response(granted)
        }
    }

    private func transcribeAudio(_ buffer: AVAudioPCMBuffer) async {
        if (!canTranscribe) {
            return
        }
        guard let whisperContext else {
            return
        }

        canTranscribe = false
        DispatchQueue.main.async {
            self.messageLog += "Transcribing data...\n"
        }
        let data = Array(UnsafeBufferPointer(start: buffer.floatChannelData![0], count: Int(buffer.frameLength)))
        print("Transcribing data with length: \(data.count)")
        await whisperContext.fullTranscribe(samples: data)
        let text = await whisperContext.getTranscription()
        DispatchQueue.main.async {
            self.messageLog += "Done: \(text)\n"
            self.transcribedText = text
            self.conversationHistory.append("User: \(text)")
        }

        canTranscribe = true
    }
}

